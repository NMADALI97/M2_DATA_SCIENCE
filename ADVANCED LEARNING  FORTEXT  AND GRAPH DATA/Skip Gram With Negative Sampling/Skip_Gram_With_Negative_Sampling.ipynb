{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Skip Gram With Negative Sampling.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDINMy7hksMC",
        "colab_type": "code",
        "outputId": "ea246460-5feb-4f99-859f-b88d8e1a1aff",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import zipfile, os\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "zip_id = '1VBT1p4nIoC9HLLYCOCzPGWuAedV50A31'\n",
        "print (\"Downloading zip file\")\n",
        "myzip = drive.CreateFile({'id': zip_id})\n",
        "myzip.GetContentFile('model.zip')\n",
        "print (\"Uncompressing zip file\")\n",
        "zip_ref = zipfile.ZipFile('model.zip', 'r')\n",
        "zip_ref.extractall()\n",
        "zip_ref.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading zip file\n",
            "Uncompressing zip file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QPP1DBowb5U",
        "colab_type": "code",
        "outputId": "bfb00596-09db-40a4-f60d-08edaaa845c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from tqdm import tqdm\n",
        "from itertools import islice\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics.pairwise import cosine_similarity as cosine"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D5xbaUgPwcyx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_windows(seq,n):\n",
        "    '''\n",
        "    returns a sliding window (of width n) over data from the iterable\n",
        "    taken from: https://stackoverflow.com/questions/6822725/rolling-or-sliding-window-iterator/6822773#6822773\n",
        "    '''\n",
        "    it = iter(seq)\n",
        "    result = tuple(islice(it, n))\n",
        "    if len(result) == n:\n",
        "        yield result\n",
        "    for elem in it:\n",
        "        result = result[1:] + (elem,)\n",
        "        yield result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rsj-mmRVw8zH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_examples(docs,max_window_size,n_windows):\n",
        "    '''generate target,context pairs and negative examples'''\n",
        "    windows = []\n",
        "    \n",
        "    for i,doc in enumerate(docs):\n",
        "       window_size=int(np.random.randint(1, high=max_window_size, size=1)[0] ) \n",
        "       \n",
        "       windows.append(get_windows(doc, window_size))\n",
        "     \n",
        "        \n",
        "         \n",
        "    windows = [list(elt) for sublist in windows for elt in sublist] # flatten\n",
        "    windows = list(np.random.choice(windows,size=n_windows)) # select a subset\n",
        "    \n",
        "   \n",
        "    \n",
        "    all_negs =np.random.choice(np.arange(1,len(vocab)+1), size=n_negs*len(windows), p=neg_distr) \n",
        "    \n",
        "    return windows,all_negs.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gplWLCv_0vb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_dot_products(pos,negs,target):\n",
        "    prods = Wc[pos+negs,] @ Wt[target,] # (n_pos+n_negs,d) X (d,) -> (n_pos+n_negs,)\n",
        "    return prods\n",
        "\n",
        "def compute_loss(prodpos,prodnegs):\n",
        "    '''prodpos and prodnegs are numpy vectors containing the dot products of the context word vectors with the target word vector'''\n",
        "    \n",
        "    term_pos=[np.log(1+np.exp(-u)) for u in prodpos]\n",
        "    term_negs=[np.log(1+np.exp(u)) for u in prodnegs]\n",
        "    return np.sum(term_pos) + np.sum(term_negs)\n",
        "    \n",
        "def compute_gradients(pos,negs,target,prodpos,prodnegs):\n",
        "    factors_pos = 1/(np.exp(prodpos)+1)\n",
        "    factors_negs = 1/(np.exp(-prodnegs)+1)\n",
        "    \n",
        "    \n",
        "    term_pos=[-Wc[pos[i],] * factors_pos[i]  for i in range(len(factors_pos)) ]\n",
        "    term_negs=[Wc[negs[i],] * factors_negs[i]  for i in range(len(factors_negs)) ]\n",
        "    \n",
        "    partial_target = np.sum(term_pos,axis=0) + np.sum(term_negs,axis=0)\n",
        "    \n",
        "    partials_pos=[-Wt[target,] * factors_pos[i]  for i in range(len(factors_pos)) ]\n",
        "    partials_negs=[Wt[target,] * factors_negs[i]  for i in range(len(factors_negs)) ]\n",
        "    \n",
        "    return partials_pos,partials_negs,partial_target\n",
        "\n",
        "def my_cos_similarity(word1,word2):\n",
        "    sim = cosine(Wt[vocab[word1],].reshape(1,-1),Wt[vocab[word2],].reshape(1,-1))\n",
        "    return round(float(sim),4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBaVk_JzxXt2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_read =\"data/\"  \n",
        "path_write = path_read\n",
        "\n",
        "stpwds = set(stopwords.words('english'))\n",
        "\n",
        "max_window_size = 5 # extends on both sides of the target word\n",
        "n_windows = int(1e6) # number of windows to sample at each epoch\n",
        "n_negs = 5 # number of negative examples to sample for each positive\n",
        "d = 30 # dimension of the embedding space\n",
        "n_epochs = 15\n",
        "lr_0 = 0.025\n",
        "decay = 1e-6\n",
        "\n",
        "train = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2h3zEnxxInN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\n",
        "with open(path_read + 'doc_ints.txt', 'r') as file:\n",
        "    docs = file.read().splitlines()\n",
        "\n",
        "docs = [[int(eltt) for eltt in elt.split()] for elt in docs]\n",
        "\n",
        "with open(path_read + 'vocab.json', 'r') as file:\n",
        "    vocab = json.load(file)\n",
        "\n",
        "vocab_inv = {v:k for k,v in vocab.items()}\n",
        "\n",
        "with open(path_read + 'counts.json', 'r') as file:\n",
        "    counts = json.load(file)\n",
        "\n",
        "token_ints = range(1,len(vocab)+1)\n",
        "neg_distr = [counts[vocab_inv[elt]] for elt in token_ints]\n",
        "neg_distr = np.sqrt(neg_distr)\n",
        "neg_distr = neg_distr/sum(neg_distr) # normalize\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyyPoX8-yTyx",
        "colab_type": "code",
        "outputId": "f5ab84dd-af5c-40b8-87c7-9510809968a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "\n",
        "# ========== train model ==========\n",
        "\n",
        "if train:\n",
        "    \n",
        "    total_its = 0\n",
        "    \n",
        "    Wt = np.random.normal(size=(len(vocab)+1,d)) # + 1 is for the OOV token\n",
        "    Wc = np.random.normal(size=(len(vocab)+1,d))\n",
        "    \n",
        "    for epoch in range(n_epochs):\n",
        "        \n",
        "        windows,all_negs = sample_examples(docs,max_window_size,n_windows)\n",
        "        print('training examples sampled')\n",
        "        \n",
        "        np.random.shuffle(windows)\n",
        "        \n",
        "        total_loss = 0\n",
        "        \n",
        "        with tqdm(total=len(windows),unit_scale=True,postfix={'loss':0.0,'lr':lr_0},desc=\"Epoch : %i/%i\" % (epoch+1, n_epochs),ncols=50) as pbar:\n",
        "            for i,w in enumerate(windows):\n",
        "                \n",
        "                target = w[int(len(w)/2)] # elt at the center\n",
        "                pos = list(w)\n",
        "                del pos[int(len(w)/2)] # all elts but the center one\n",
        "                \n",
        "                negs = all_negs[n_negs*i:n_negs*i+n_negs]\n",
        "                \n",
        "                prods = compute_dot_products(pos,negs,target)\n",
        "                prodpos = prods[0:len(pos),]\n",
        "                prodnegs = prods[len(pos):(len(pos)+len(negs)),]\n",
        "                \n",
        "                partials_pos,partials_negs,partial_target = compute_gradients(pos,negs,target,prodpos,prodnegs)\n",
        "                \n",
        "                lr = lr_0 * 1/(1+decay*total_its)\n",
        "                total_its += 1\n",
        "                \n",
        "                \n",
        "                Wt[target,] -=lr*np.array(partial_target)\n",
        "                if len(pos) > 0:\n",
        "                 Wc[pos,] -=lr*np.array(partials_pos)\n",
        "                Wc[negs,] -=lr*np.array(partials_negs)\n",
        "                \n",
        "                total_loss += compute_loss(prodpos,prodnegs)\n",
        "                pbar.set_postfix({'loss':total_loss/(i+1),'lr':lr})\n",
        "                pbar.update(1)\n",
        "                \n",
        "\n",
        "        np.save(path_write + 'input_vecs_'+str(epoch+1),Wt,allow_pickle=False) # pickle disabled for portability reasons\n",
        "        np.save(path_write + 'output_vecs_'+str(epoch+1),Wc,allow_pickle=False)\n",
        "    \n",
        "    print('word vectors saved to disk')\n",
        "    \n",
        "else:\n",
        "    Wt = np.load(path_write + 'input_vecs.npy')\n",
        "    Wc = np.load(path_write + 'output_vecs.npy')\n",
        "    \n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1/15:   0%| | 1.00/1.00M [00:00<2:04:10, 134it/s, loss=10.9, lr=0.025]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "training examples sampled\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch : 1/15:   2%| | 15.5k/1.00M [00:52<56:43, 289it/s, loss=10.7, lr=0.0246]"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW2USJHHI3R7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# ========== sanity checks ==========\n",
        "\n",
        "if not train:\n",
        "\n",
        "    # = = some similarities = = \n",
        "    \n",
        "\n",
        "    # = = visualization of most frequent tokens = =\n",
        "\n",
        "    n_plot = 500\n",
        "    mft = [vocab_inv[elt] for elt in range(1,n_plot+1)]\n",
        "\n",
        "    # exclude stopwords and punctuation\n",
        "    keep_idxs = [idx for idx,elt in enumerate(mft) if len(elt)>3 and elt not in stpwds]\n",
        "    mft = [mft[idx] for idx in keep_idxs]\n",
        "    keep_ints = [list(range(1,n_plot+1))[idx] for idx in keep_idxs]\n",
        "    Wt_freq = Wt[keep_ints,]\n",
        "    \n",
        "    \n",
        "    ### for t-SNE, see https://lvdmaaten.github.io/tsne/#faq ###\n",
        "    my_pca = PCA(n_components=10)\n",
        "    my_tsne = TSNE(n_components=2,perplexity=5)\n",
        "\n",
        "    my_pca_fit = my_pca.fit_transform(Wt_freq)\n",
        "    my_tsne_fit = my_tsne.fit_transform(my_pca_fit)\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    ax.scatter(,s=3) \n",
        "    for x,y,token in zip(,mft): \n",
        "        ax.annotate(token, xy=(x,y), size=8)\n",
        "\n",
        "    fig.suptitle('t-SNE visualization of word embeddings',fontsize=20)\n",
        "    fig.set_size_inches(11,7)\n",
        "    fig.savefig(path_write + 'word_embeddings.pdf',dpi=300)\n",
        "    fig.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}